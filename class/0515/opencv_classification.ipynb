{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1794ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\Computer_Vision\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "Downloading https://ultralytics.com/images/bus.jpg to 'bus.jpg'...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 476k/476k [00:00<00:00, 1.02MB/s]\n",
      "image 1/1 D:\\Computer_Vision\\class\\0515\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 535.1ms\n",
      "Speed: 24.9ms preprocess, 535.1ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#YOLOv8 Classification\n",
    "#pip install ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\" , save=True)  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae02757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 486.0ms\n",
      "Speed: 5.0ms preprocess, 486.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cup, 1 keyboard, 406.5ms\n",
      "Speed: 6.0ms preprocess, 406.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14.4951,   1.0375, 524.7303, 359.1754]])\n",
      "14 1 524 359\n",
      "tensor([[  1.1997, 332.9853, 450.9841, 413.9505]])\n",
      "1 332 450 413\n",
      "tensor([[498.9273, 132.4090, 639.7971, 357.1010]])\n",
      "498 132 639 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 621.2ms\n",
      "Speed: 7.0ms preprocess, 621.2ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57.4367,   0.0000, 525.5173, 361.8698]])\n",
      "57 0 525 361\n",
      "tensor([[  0.4278, 331.2693, 427.6001, 410.0147]])\n",
      "0 331 427 410\n",
      "tensor([[  0.7789, 332.8336, 639.4015, 428.5322]])\n",
      "0 332 639 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 362.6ms\n",
      "Speed: 12.3ms preprocess, 362.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3441e+01, 2.3676e-01, 5.4443e+02, 3.6106e+02]])\n",
      "43 0 544 361\n",
      "tensor([[4.3048e-01, 3.3203e+02, 4.3482e+02, 4.0543e+02]])\n",
      "0 332 434 405\n",
      "tensor([[  2.0720, 331.4066, 639.5984, 428.1846]])\n",
      "2 331 639 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 454.5ms\n",
      "Speed: 7.0ms preprocess, 454.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 45.6898,   0.5630, 533.1709, 359.9066]])\n",
      "45 0 533 359\n",
      "tensor([[  0.5806, 332.3620, 457.7258, 406.4760]])\n",
      "0 332 457 406\n",
      "tensor([[  3.9765, 339.6194, 639.8061, 421.6728]])\n",
      "3 339 639 421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 331.1ms\n",
      "Speed: 6.0ms preprocess, 331.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6459e+01, 4.3372e-01, 5.4786e+02, 3.6278e+02]])\n",
      "26 0 547 362\n",
      "tensor([[  1.2313, 331.7412, 402.6336, 407.0311]])\n",
      "1 331 402 407\n",
      "tensor([[  1.0170, 333.4573, 638.8832, 432.9255]])\n",
      "1 333 638 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 425.9ms\n",
      "Speed: 7.0ms preprocess, 425.9ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 40.4927,   0.0000, 561.7373, 363.5919]])\n",
      "40 0 561 363\n",
      "tensor([[  0.4321, 332.1569, 398.1884, 404.2536]])\n",
      "0 332 398 404\n",
      "tensor([[  0.0000, 334.8349, 613.6946, 416.9216]])\n",
      "0 334 613 416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 1 keyboard, 339.1ms\n",
      "Speed: 7.0ms preprocess, 339.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3480e+01, 5.1355e-01, 5.3261e+02, 3.5962e+02]])\n",
      "23 0 532 359\n",
      "tensor([[  1.5794, 332.6299, 514.4758, 413.5966]])\n",
      "1 332 514 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 309.7ms\n",
      "Speed: 3.0ms preprocess, 309.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50.0420,   1.0793, 524.5593, 354.6973]])\n",
      "50 1 524 354\n",
      "tensor([[  0.4421, 330.9036, 416.8078, 407.6597]])\n",
      "0 330 416 407\n",
      "tensor([[  4.9692, 341.0447, 640.0000, 420.6089]])\n",
      "4 341 640 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 294.2ms\n",
      "Speed: 4.0ms preprocess, 294.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 47.6346,   2.6809, 529.0189, 364.8982]])\n",
      "47 2 529 364\n",
      "tensor([[  0.4663, 332.1002, 403.6017, 405.8344]])\n",
      "0 332 403 405\n",
      "tensor([[  2.0291, 331.5338, 547.1880, 414.5905]])\n",
      "2 331 547 414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 302.2ms\n",
      "Speed: 7.0ms preprocess, 302.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 49.2817,   2.4982, 514.8207, 358.7042]])\n",
      "49 2 514 358\n",
      "tensor([[  0.7455, 331.5558, 436.2049, 404.7783]])\n",
      "0 331 436 404\n",
      "tensor([[3.6615e-01, 3.3755e+02, 6.4000e+02, 4.3022e+02]])\n",
      "0 337 640 430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 364.0ms\n",
      "Speed: 8.0ms preprocess, 364.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.8635e+01, 3.1876e-01, 5.5753e+02, 3.6710e+02]])\n",
      "38 0 557 367\n",
      "tensor([[  0.6598, 331.1754, 450.7448, 411.4106]])\n",
      "0 331 450 411\n",
      "tensor([[  1.7671, 335.7587, 640.0000, 414.4851]])\n",
      "1 335 640 414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 362.1ms\n",
      "Speed: 15.0ms preprocess, 362.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 54.3694,   1.1538, 597.9576, 363.1435]])\n",
      "54 1 597 363\n",
      "tensor([[  0.6179, 332.4188, 431.2454, 406.4610]])\n",
      "0 332 431 406\n",
      "tensor([[  0.9731, 337.6719, 640.0000, 418.0867]])\n",
      "0 337 640 418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 300.2ms\n",
      "Speed: 4.0ms preprocess, 300.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 37.2806,   1.7401, 573.8551, 369.6096]])\n",
      "37 1 573 369\n",
      "tensor([[  0.5646, 332.1410, 443.2630, 410.1425]])\n",
      "0 332 443 410\n",
      "tensor([[  3.3140, 339.5142, 640.0000, 421.6809]])\n",
      "3 339 640 421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 292.2ms\n",
      "Speed: 4.0ms preprocess, 292.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57.8177,   1.1301, 611.2139, 376.3822]])\n",
      "57 1 611 376\n",
      "tensor([[  0.5645, 332.1559, 445.6667, 410.2110]])\n",
      "0 332 445 410\n",
      "tensor([[3.7427e-01, 3.3116e+02, 6.1818e+02, 4.2461e+02]])\n",
      "0 331 618 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 288.7ms\n",
      "Speed: 7.0ms preprocess, 288.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 42.0582,   1.2169, 597.5853, 367.0310]])\n",
      "42 1 597 367\n",
      "tensor([[  0.5869, 331.4472, 435.4838, 412.8809]])\n",
      "0 331 435 412\n",
      "tensor([[  0.8727, 335.8755, 640.0000, 419.8757]])\n",
      "0 335 640 419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 355.6ms\n",
      "Speed: 7.0ms preprocess, 355.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 47.6880,   1.1813, 573.4406, 361.6923]])\n",
      "47 1 573 361\n",
      "tensor([[  0.5378, 331.7216, 447.4998, 408.5811]])\n",
      "0 331 447 408\n",
      "tensor([[  4.8061, 339.8619, 639.7431, 418.6471]])\n",
      "4 339 639 418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 292.7ms\n",
      "Speed: 7.0ms preprocess, 292.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 33.3296,   1.5632, 590.9714, 367.4719]])\n",
      "33 1 590 367\n",
      "tensor([[  1.6326, 334.6691, 640.0000, 417.1328]])\n",
      "1 334 640 417\n",
      "tensor([[  0.5663, 331.9673, 411.6721, 407.4595]])\n",
      "0 331 411 407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 289.2ms\n",
      "Speed: 9.0ms preprocess, 289.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.0801,   0.6935, 589.3120, 374.0099]])\n",
      "56 0 589 374\n",
      "tensor([[  0.5608, 332.2816, 476.7002, 405.8185]])\n",
      "0 332 476 405\n",
      "tensor([[  5.4612, 340.8344, 639.4777, 418.9467]])\n",
      "5 340 639 418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 311.7ms\n",
      "Speed: 7.0ms preprocess, 311.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 40.1199,   1.3483, 586.1220, 366.5460]])\n",
      "40 1 586 366\n",
      "tensor([[  0.5047, 332.6569, 408.1354, 405.5779]])\n",
      "0 332 408 405\n",
      "tensor([[  1.4206, 335.4365, 640.0000, 416.2056]])\n",
      "1 335 640 416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 299.2ms\n",
      "Speed: 5.0ms preprocess, 299.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.1631e+01, 3.1845e-01, 5.5111e+02, 3.6049e+02]])\n",
      "51 0 551 360\n",
      "tensor([[  0.6315, 331.3934, 474.1183, 415.6541]])\n",
      "0 331 474 415\n",
      "tensor([[  1.6951, 340.6926, 640.0000, 422.3241]])\n",
      "1 340 640 422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 282.3ms\n",
      "Speed: 4.0ms preprocess, 282.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 28.6202,   0.0000, 534.7079, 364.8906]])\n",
      "28 0 534 364\n",
      "tensor([[  0.5234, 331.1763, 476.5544, 416.1917]])\n",
      "0 331 476 416\n",
      "tensor([[  1.9830, 335.2362, 640.0000, 417.9125]])\n",
      "1 335 640 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 286.7ms\n",
      "Speed: 11.0ms preprocess, 286.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 51.7392,   0.8460, 513.9304, 356.9515]])\n",
      "51 0 513 356\n",
      "tensor([[  0.5261, 331.2499, 465.2854, 417.0819]])\n",
      "0 331 465 417\n",
      "tensor([[  1.9949, 339.0797, 640.0000, 425.4233]])\n",
      "1 339 640 425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 285.2ms\n",
      "Speed: 8.0ms preprocess, 285.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 48.5943,   0.0000, 510.1404, 353.9104]])\n",
      "48 0 510 353\n",
      "tensor([[  0.5600, 331.5117, 426.3686, 413.2901]])\n",
      "0 331 426 413\n",
      "tensor([[  1.4343, 335.8716, 640.0000, 418.3781]])\n",
      "1 335 640 418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 300.7ms\n",
      "Speed: 9.0ms preprocess, 300.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.4384e+01, 3.0191e-01, 5.5482e+02, 3.5927e+02]])\n",
      "34 0 554 359\n",
      "tensor([[6.1890e-01, 3.3417e+02, 6.3926e+02, 4.4158e+02]])\n",
      "0 334 639 441\n",
      "tensor([[  0.5188, 331.0288, 462.2134, 412.7095]])\n",
      "0 331 462 412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 315.2ms\n",
      "Speed: 6.0ms preprocess, 315.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50.2416,   0.5321, 518.9005, 354.9004]])\n",
      "50 0 518 354\n",
      "tensor([[  0.4925, 330.7905, 440.7215, 413.8171]])\n",
      "0 330 440 413\n",
      "tensor([[  2.7285, 332.6278, 596.2950, 423.7166]])\n",
      "2 332 596 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 289.2ms\n",
      "Speed: 5.0ms preprocess, 289.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50.4891,   0.9745, 531.4077, 362.2303]])\n",
      "50 0 531 362\n",
      "tensor([[  0.5671, 331.7377, 458.1943, 406.4651]])\n",
      "0 331 458 406\n",
      "tensor([[  2.1506, 333.3620, 580.0846, 424.4234]])\n",
      "2 333 580 424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 287.7ms\n",
      "Speed: 4.0ms preprocess, 287.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 55.4550,   1.0298, 515.6289, 355.6089]])\n",
      "55 1 515 355\n",
      "tensor([[  0.5329, 331.4083, 417.3629, 408.7719]])\n",
      "0 331 417 408\n",
      "tensor([[2.0276e-01, 3.3219e+02, 6.3983e+02, 4.2845e+02]])\n",
      "0 332 639 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 299.2ms\n",
      "Speed: 4.0ms preprocess, 299.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 47.5201,   1.0051, 548.9156, 362.1961]])\n",
      "47 1 548 362\n",
      "tensor([[  0.5182, 331.6243, 424.6895, 412.8488]])\n",
      "0 331 424 412\n",
      "tensor([[  0.0000, 335.2504, 639.7819, 430.3438]])\n",
      "0 335 639 430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 302.7ms\n",
      "Speed: 6.0ms preprocess, 302.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 46.0318,   0.9110, 538.3695, 360.1722]])\n",
      "46 0 538 360\n",
      "tensor([[  0.6234, 331.6467, 454.0021, 410.0708]])\n",
      "0 331 454 410\n",
      "tensor([[5.5035e-01, 3.3353e+02, 6.3970e+02, 4.3155e+02]])\n",
      "0 333 639 431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 1 keyboard, 273.3ms\n",
      "Speed: 7.0ms preprocess, 273.3ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 51.4176,   1.3920, 512.2371, 356.8557]])\n",
      "51 1 512 356\n",
      "tensor([[  0.4325, 331.2148, 430.2378, 410.0185]])\n",
      "0 331 430 410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 286.7ms\n",
      "Speed: 5.0ms preprocess, 286.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50.2229,   1.9858, 512.6377, 355.1247]])\n",
      "50 1 512 355\n",
      "tensor([[  0.6025, 331.1154, 475.1950, 415.5295]])\n",
      "0 331 475 415\n",
      "tensor([[  1.4659, 336.8557, 640.0000, 415.2979]])\n",
      "1 336 640 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 374.0ms\n",
      "Speed: 6.0ms preprocess, 374.0ms inference, 18.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 48.0787,   0.6303, 570.2433, 365.0987]])\n",
      "48 0 570 365\n",
      "tensor([[  0.4853, 330.7632, 469.6505, 411.5789]])\n",
      "0 330 469 411\n",
      "tensor([[  3.1623, 335.5983, 640.0000, 422.4375]])\n",
      "3 335 640 422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 388.0ms\n",
      "Speed: 5.0ms preprocess, 388.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 39.4070,   1.2854, 550.8613, 358.1499]])\n",
      "39 1 550 358\n",
      "tensor([[  0.5489, 330.9285, 459.0804, 410.4468]])\n",
      "0 330 459 410\n",
      "tensor([[  2.6451, 332.9634, 607.7883, 432.2184]])\n",
      "2 332 607 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 342.1ms\n",
      "Speed: 5.0ms preprocess, 342.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 48.9807,   1.1717, 555.4160, 360.9150]])\n",
      "48 1 555 360\n",
      "tensor([[  0.5385, 330.4208, 502.9276, 418.1483]])\n",
      "0 330 502 418\n",
      "tensor([[  6.7582, 339.7473, 639.4183, 421.7837]])\n",
      "6 339 639 421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 295.2ms\n",
      "Speed: 4.0ms preprocess, 295.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.5580e+01, 6.9305e-02, 5.5115e+02, 3.6352e+02]])\n",
      "65 0 551 363\n",
      "tensor([[  0.6992, 332.2346, 424.3977, 408.1460]])\n",
      "0 332 424 408\n",
      "tensor([[  1.3326, 335.2369, 640.0000, 419.8247]])\n",
      "1 335 640 419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 1 chair, 3 keyboards, 287.2ms\n",
      "Speed: 4.0ms preprocess, 287.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.3930e+01, 1.1722e-01, 5.8070e+02, 3.6847e+02]])\n",
      "53 0 580 368\n",
      "tensor([[  0.4749, 332.3427, 421.6941, 404.6927]])\n",
      "0 332 421 404\n",
      "tensor([[  5.5859, 339.5314, 639.4122, 421.2059]])\n",
      "5 339 639 421\n",
      "tensor([[  2.0662, 330.5584, 573.7052, 408.0768]])\n",
      "2 330 573 408\n",
      "tensor([[ 35.7975, 225.8548, 111.2858, 335.5005]])\n",
      "35 225 111 335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 2 keyboards, 282.8ms\n",
      "Speed: 3.0ms preprocess, 282.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 65.2834,   1.1251, 529.4443, 360.7604]])\n",
      "65 1 529 360\n",
      "tensor([[  2.2452, 335.2304, 640.0000, 420.4541]])\n",
      "2 335 640 420\n",
      "tensor([[1.9904e-01, 3.3350e+02, 4.3778e+02, 4.1946e+02]])\n",
      "0 333 437 419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 3 keyboards, 292.2ms\n",
      "Speed: 8.0ms preprocess, 292.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.8519,   0.6808, 593.3256, 373.0809]])\n",
      "1 0 593 373\n",
      "tensor([[1.1093e-01, 3.3468e+02, 3.9375e+02, 4.0767e+02]])\n",
      "0 334 393 407\n",
      "tensor([[  0.6330, 333.9727, 551.3941, 412.8098]])\n",
      "0 333 551 412\n",
      "tensor([[  0.8573, 320.5051, 639.9390, 434.4242]])\n",
      "0 320 639 434\n"
     ]
    }
   ],
   "source": [
    "#YOLOv8 Classification\n",
    "#pip install ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "cap = cv2.VideoCapture(0)\n",
    "#cap.set(3, 640)\n",
    "#cap.set(4, 480)\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    img = cv2.flip(img, 1)\n",
    "    # BGR to RGB conversion is performed under the hood\n",
    "    # see: https://github.com/ultralytics/ultralytics/issues/2575\n",
    "    results = model.predict(img, conf=0.25)  # Set your desired confidence threshold, default = 0.25\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            print(box.xyxy) #box.xyxy.cpu()\n",
    "            left, top, right, bottom = np.array(box.xyxy, dtype=np.uint16).squeeze() #convert from tensor to list\n",
    "            print(left, top, right, bottom)\n",
    "            width = right - left\n",
    "            height = bottom - top\n",
    "            center = (left + int((right-left)/2), top + int((bottom-top)/2))\n",
    "            label = results[0].names[int(box.cls)]\n",
    "            confidence = float(box.conf.cpu())\n",
    "\n",
    "            cv2.rectangle(img, (left, top),(right, bottom), (255, 0, 0), 2)\n",
    "            cv2.putText(img, label+' '+'{:.2f}'.format(confidence),(left+5, bottom-10),cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('YOLO V8 Detection', img)     \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857d491",
   "metadata": {},
   "source": [
    "## Practice : People detector and analyzer\n",
    "1. Input images from wiiplay.mp4 with frame number between 41000 and 44000.\n",
    "2. Use YOLOv8 to detect people, mark as red rectangle, and count how many persons in each frame. (hint: check label == 'person')\n",
    "3. Try to find out which frame contains the most number of persons. (print the number of persons on the upper-left corner)\n",
    "4. (optional) Try to find out which frame containes the largest person. (print the size of its bounding box on the upper-left corner)\n",
    "5. (optional) Try to find out which frame containes the smallest person. (print the size of its bounding box on the upper-left corner)\n",
    "6. Show the three output frames you found.\n",
    "7. Verify the correctness of your output, then adjust the desired confidence threshold for improvement. \n",
    "8. Upload your Jupyter code file (*.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer_Vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
