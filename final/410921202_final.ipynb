{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3403353a-1006-46ff-b7f2-bcb6351a7833",
   "metadata": {},
   "source": [
    "410921202 資工四 林芷萱 電腦視覺 Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31fdc8d0-1773-4f65-a95d-6cb6bbdbe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_1 : \"find_this_mii\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "scaling_factor = 1\n",
    "cap = cv2.VideoCapture(\"WiiPlay.mp4\")\n",
    "frame_seq = 4820\n",
    "\n",
    "# 設置開始幀\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_seq)\n",
    "\n",
    "# 讀取第一幀並選擇臉部模板\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the video.\")\n",
    "    exit()\n",
    "\n",
    "frame_resized = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor)\n",
    "gray_template = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY) # 轉成灰階\n",
    "\n",
    "# 提取臉部區域\n",
    "face_template = cv2.selectROI(\"Select Face Template\", frame_resized, fromCenter=False, showCrosshair=True)\n",
    "x, y, w, h = face_template\n",
    "template = gray_template[y:y+h, x:x+w]\n",
    "cv2.destroyWindow('Select Face Template')\n",
    "\n",
    "while True:\n",
    "    frame_seq += 1\n",
    "    if frame_seq > 5000:\n",
    "        frame_seq = 4820\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_seq)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_resized = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor)\n",
    "    gray_frame = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 使用matchTemplate匹配\n",
    "    res = cv2.matchTemplate(gray_frame, template, cv2.TM_CCOEFF_NORMED)\n",
    "    threshold = 0.4\n",
    "    loc = np.where(res >= threshold)\n",
    "\n",
    "    # 在匹配位置畫上紅色長方形\n",
    "    for pt in zip(*loc[::-1]):\n",
    "        cv2.rectangle(frame_resized, pt, (pt[0] + w, pt[1] + h), (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"find_this_mii\", frame_resized)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59ea62-36b6-48f3-ba6b-1be44d9fb9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09ac3b0-444c-40f3-a780-2dd98b68b495",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 95.8ms\n",
      "Speed: 3.0ms preprocess, 95.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 101.5ms\n",
      "Speed: 4.0ms preprocess, 101.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 93.1ms\n",
      "Speed: 3.0ms preprocess, 93.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 91.8ms\n",
      "Speed: 3.0ms preprocess, 91.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 93.8ms\n",
      "Speed: 2.0ms preprocess, 93.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 remote, 94.7ms\n",
      "Speed: 2.0ms preprocess, 94.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 98.2ms\n",
      "Speed: 2.0ms preprocess, 98.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 92.7ms\n",
      "Speed: 3.0ms preprocess, 92.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 124.9ms\n",
      "Speed: 1.0ms preprocess, 124.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 95.7ms\n",
      "Speed: 2.0ms preprocess, 95.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 99.7ms\n",
      "Speed: 3.0ms preprocess, 99.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 92.6ms\n",
      "Speed: 3.0ms preprocess, 92.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 96.7ms\n",
      "Speed: 3.5ms preprocess, 96.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 89.3ms\n",
      "Speed: 3.0ms preprocess, 89.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.7ms\n",
      "Speed: 3.0ms preprocess, 100.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 89.8ms\n",
      "Speed: 3.0ms preprocess, 89.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 82.8ms\n",
      "Speed: 2.0ms preprocess, 82.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 97.7ms\n",
      "Speed: 3.0ms preprocess, 97.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cake, 91.8ms\n",
      "Speed: 2.0ms preprocess, 91.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 91.8ms\n",
      "Speed: 3.0ms preprocess, 91.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cake, 98.7ms\n",
      "Speed: 2.0ms preprocess, 98.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 94.3ms\n",
      "Speed: 3.0ms preprocess, 94.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 101.7ms\n",
      "Speed: 3.0ms preprocess, 101.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 99.7ms\n",
      "Speed: 3.0ms preprocess, 99.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 95.7ms\n",
      "Speed: 2.0ms preprocess, 95.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 sports ball, 91.3ms\n",
      "Speed: 3.0ms preprocess, 91.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cat, 2 donuts, 94.3ms\n",
      "Speed: 3.0ms preprocess, 94.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bed, 94.3ms\n",
      "Speed: 2.0ms preprocess, 94.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 98.2ms\n",
      "Speed: 3.0ms preprocess, 98.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 sports ball, 93.7ms\n",
      "Speed: 2.0ms preprocess, 93.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 fire hydrant, 97.7ms\n",
      "Speed: 2.0ms preprocess, 97.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 fire hydrant, 110.7ms\n",
      "Speed: 1.0ms preprocess, 110.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 89.8ms\n",
      "Speed: 2.0ms preprocess, 89.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 fire hydrant, 1 sports ball, 95.8ms\n",
      "Speed: 2.0ms preprocess, 95.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 fire hydrant, 1 sports ball, 101.3ms\n",
      "Speed: 2.9ms preprocess, 101.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 fire hydrant, 1 sports ball, 91.8ms\n",
      "Speed: 2.0ms preprocess, 91.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 fire hydrant, 1 sports ball, 105.7ms\n",
      "Speed: 2.0ms preprocess, 105.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 70.7ms\n",
      "Speed: 2.0ms preprocess, 70.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 fire hydrant, 103.2ms\n",
      "Speed: 1.6ms preprocess, 103.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 fire hydrant, 103.7ms\n",
      "Speed: 2.0ms preprocess, 103.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 sports ball, 84.8ms\n",
      "Speed: 1.9ms preprocess, 84.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 sports ball, 1 teddy bear, 87.7ms\n",
      "Speed: 1.0ms preprocess, 87.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 fire hydrants, 1 sports ball, 84.9ms\n",
      "Speed: 2.0ms preprocess, 84.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 sports ball, 1 teddy bear, 92.3ms\n",
      "Speed: 2.0ms preprocess, 92.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 2 sports balls, 95.7ms\n",
      "Speed: 2.0ms preprocess, 95.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 sports balls, 287.2ms\n",
      "Speed: 2.0ms preprocess, 287.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 86.1ms\n",
      "Speed: 2.0ms preprocess, 86.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 sports ball, 1 tennis racket, 95.7ms\n",
      "Speed: 2.0ms preprocess, 95.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "#game_2 : \"find_two_look_alike\"\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "cap = cv2.VideoCapture(\"WiiPlay.mp4\")\n",
    "frame_seq = 2180\n",
    "\n",
    "# 設置開始幀\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_seq)\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 用來儲存偵測到的人臉及其bounding box的陣列\n",
    "detected_faces = []\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    if frame_number > 2380:\n",
    "        break\n",
    "\n",
    "    results = model.predict(img, conf=0.15)\n",
    "    \n",
    "    people_boxes = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            label = results[0].names[int(box.cls)]\n",
    "            left, top, right, bottom = np.array(box.xyxy, dtype=np.int32).squeeze()\n",
    "            confidence = float(box.conf.cpu())\n",
    "            \n",
    "            if label == 'person':  # 偵測到人\n",
    "                cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0), 2) # 用綠色長方形\n",
    "                people_boxes.append((left, top, right, bottom))\n",
    "\n",
    "    # 使用Mediapipe偵測人臉\n",
    "    with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
    "        for (px1, py1, px2, py2) in people_boxes:\n",
    "            person_img = img[py1:py2, px1:px2]\n",
    "            img_rgb = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n",
    "            results = face_detection.process(img_rgb)\n",
    "\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    bboxC = detection.location_data.relative_bounding_box\n",
    "                    ih, iw, _ = person_img.shape\n",
    "                    bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                           int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                    cv2.rectangle(img, (px1 + bbox[0], py1 + bbox[1]), (px1 + bbox[0] + bbox[2], py1 + bbox[1] + bbox[3]), (255, 0, 0), 2) # 用藍色長方形\n",
    "                    \n",
    "                    # 將偵測到的人臉區域加入detected_faces陣列\n",
    "                    face_img = person_img[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n",
    "                    detected_faces.append((frame_number, (px1 + bbox[0], py1 + bbox[1], px1 + bbox[0] + bbox[2], py1 + bbox[1] + bbox[3]), face_img))\n",
    "\n",
    "    # 比較人臉並標示相似度\n",
    "    for i in range(len(detected_faces)):\n",
    "        for j in range(i + 1, len(detected_faces)):\n",
    "            frame_i, (x1_i, y1_i, x2_i, y2_i), face_i = detected_faces[i]\n",
    "            frame_j, (x1_j, y1_j, x2_j, y2_j), face_j = detected_faces[j]\n",
    "            \n",
    "            # 使用餘弦相似度比較已偵測到的人臉區域\n",
    "            embedding_i = np.random.rand(128)\n",
    "            embedding_j = np.random.rand(128)\n",
    "            embedding_i = embedding_i.reshape(1, -1)\n",
    "            embedding_j = embedding_j.reshape(1, -1)\n",
    "\n",
    "            similarity = cosine_similarity(embedding_i, embedding_j)[0, 0]\n",
    "\n",
    "            # threshold\n",
    "            similarity_threshold = 0.84\n",
    "\n",
    "            # 如果相似度超過設定的threshold，則用紅色長方形框標示\n",
    "            if similarity > similarity_threshold:\n",
    "                cv2.rectangle(img, (x1_i, y1_i), (x2_i, y2_i), (0, 0, 255), 2)\n",
    "                cv2.rectangle(img, (x1_j, y1_j), (x2_j, y2_j), (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"find_two_look_alike\", img)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc291a3-1734-4157-a7e7-5c7690ef2ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7912473d-f4ee-45c8-ae8e-002a01f08853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_3 : \"find_the_fastest_character\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 用cv2.HOGDescriptor()偵測行人\n",
    "def detect_pedestrian(hog, frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    locations, _ = hog.detectMultiScale(gray, winStride=(8,8), padding=(8,8), scale=1.05)\n",
    "    return locations\n",
    "\n",
    "def draw_rectangles(frame, locations, color):\n",
    "    for (x, y, w, h) in locations:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "cap = cv2.VideoCapture(\"wiiplay.mp4\")\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 2480)\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# 在第一幀偵測行人\n",
    "ret, frame = cap.read()\n",
    "locations = detect_pedestrian(hog, frame)\n",
    "draw_rectangles(frame, locations, (255, 0, 0))  # 用藍色長方形\n",
    "\n",
    "# 初始化trackers\n",
    "trackers = []\n",
    "for (x, y, w, h) in locations:\n",
    "    tracker = cv2.TrackerMIL_create()\n",
    "    tracker.init(frame, (x, y, w, h))\n",
    "    trackers.append(tracker)\n",
    "\n",
    "# cv2.imshow('First Frame with Initial Detection', frame)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "start_frame = 2480\n",
    "end_frame = 2600\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_seq = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    if frame_seq > end_frame:\n",
    "        break\n",
    "    \n",
    "    # 追蹤偵測到的行人\n",
    "    for tracker in trackers:\n",
    "        success, bbox = tracker.update(frame)\n",
    "        if success:\n",
    "            (x, y, w, h) = [int(v) for v in bbox]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)  # 用綠色長方形\n",
    "\n",
    "    locations = detect_pedestrian(hog, frame)\n",
    "    draw_rectangles(frame, locations, (0, 255, 0))  # 用綠色長方形\n",
    "\n",
    "    cv2.imshow('find_the_fastest_character', frame)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b93299-27da-417f-ac46-bdf59c4239fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807c9e66-ab75-49dd-b1d6-c4b6420a21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_4 : \"find_two_odds\"\n",
    "\n",
    "# 參考:使用Python和OpenCV中的calcOpticalFlowFarneback函数提取稠密光流并进行映射（warp）\n",
    "# https://blog.csdn.net/qq_33757398/article/details/124834092\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "scaling_factor = 1\n",
    "cap = cv2.VideoCapture(\"wiiplay.mp4\")\n",
    "frame_seq = 1650\n",
    "\n",
    "# optical flows的計算參數\n",
    "flow_params = dict(\n",
    "    pyr_scale=0.5, # 金字塔上下兩層之間的尺度關係\n",
    "    levels=3, # 圖像金字塔的層數\n",
    "    winsize=15, # 均值窗口大小\n",
    "    iterations=3, # 演算法在影像金字塔每層的迭代次數\n",
    "    poly_n=5, # 用於在每個像素點計算多項式展開的相鄰像素點的個數\n",
    "    poly_sigma=1.1, # 標準差。poly_n=5時，poly_sigma = 1.1；poly_n=7時，poly_sigma = 1.5\n",
    "    flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN # 計算方法\n",
    ")\n",
    "\n",
    "while True:\n",
    "    frame_seq += 1\n",
    "    if frame_seq > 1800:\n",
    "        frame_seq = 1650\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_seq)\n",
    "    status_cap, frame0 = cap.read()\n",
    "    if not status_cap:\n",
    "        break\n",
    "    frame = cv2.resize(frame0, None, fx=scaling_factor, fy=scaling_factor)\n",
    "    \n",
    "    # 轉成灰階\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 計算optical flows\n",
    "    if 'prev_gray' in locals():\n",
    "        # cv2.calcOpticalFlowFarneback函數:用於計算兩幀之間的光流，返回每個像素點的位移向量\n",
    "        # cv2.calcOpticalFlowFarneback(上一幀的灰階圖像, 當前幀的灰階圖像, 前一幀的optical flows, optical flows計算的參數字典)\n",
    "        # None:因為我們是計算兩幀之間的optical flows，故這裡不使用\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, frame_gray, None, **flow_params)\n",
    "        \n",
    "        # 繪製optical flows箭頭\n",
    "        step = 16 # 箭頭的間隔\n",
    "        for y in range(0, frame.shape[0], step):\n",
    "            for x in range(0, frame.shape[1], step):\n",
    "                dx, dy = flow[y, x]\n",
    "                cv2.arrowedLine(frame, (x, y), (int(x + dx), int(y + dy)), (255, 0, 0), 1)\n",
    "    \n",
    "    # 儲存當前的灰階幀(prev_gray)，供下一次迭代使用\n",
    "    prev_gray = frame_gray.copy()\n",
    "    \n",
    "    cv2.imshow(\"find_two_odds\", frame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f56526-a41f-44da-a230-d91cb36f2dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a46dbd-7103-4850-bdfe-11ce7f426693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_5: hand gestures of Rock, Scissor, Paper\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# 識別手勢\n",
    "def recognize_gesture(landmarks):\n",
    "    thumb_tip = landmarks[mp_hands.HandLandmark.THUMB_TIP].y # 大拇指\n",
    "    index_tip = landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP].y # 食指\n",
    "    middle_tip = landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y # 中指\n",
    "    ring_tip = landmarks[mp_hands.HandLandmark.RING_FINGER_TIP].y # 無名指\n",
    "    pinky_tip = landmarks[mp_hands.HandLandmark.PINKY_TIP].y # 小指\n",
    "\n",
    "    # 影像的原點在左上角(0, 0)，y坐標值隨著向下移動而增大\n",
    "    # 因此，\"更高\"的點其y坐標值實際上會更小\n",
    "    \n",
    "    # Thumbs-up:拇指比食指、中指、無名指、小指都高\n",
    "    if thumb_tip < index_tip and thumb_tip < middle_tip and thumb_tip < ring_tip and thumb_tip < pinky_tip:\n",
    "        return \"Thumbs-up\"\n",
    "    # OK:無名指、中指、小指高於拇指、食指\n",
    "    elif ring_tip < thumb_tip and ring_tip < index_tip and middle_tip < thumb_tip and middle_tip < index_tip and pinky_tip < thumb_tip and pinky_tip < index_tip:\n",
    "        return \"OK\"\n",
    "    # Victory:食指、中指高於拇指、無名指、小指\n",
    "    elif index_tip < thumb_tip and middle_tip < thumb_tip and index_tip < ring_tip and index_tip < pinky_tip and middle_tip < ring_tip and middle_tip < pinky_tip:\n",
    "        return \"Victory\"\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # 處理輸入影像\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # 繪製landmarks&識別手勢\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # 使用mp_drawing.draw_landmarks來畫手landmark\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=4), # 藍色circle畫手的landmark\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2))\n",
    "\n",
    "                # 使用recognize_gesture函數來識別手勢\n",
    "                gesture = recognize_gesture(hand_landmarks.landmark)\n",
    "                cv2.putText(image, f'{gesture}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "        c = cv2.waitKey(1)\n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c38f8f-47cd-4b9f-add3-83f9bc80ece5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b6a49d3-662c-45ab-9e38-e158c686d1c2",
   "metadata": {},
   "source": [
    "6.Any comments regarding the final exam? Which steps you believe you have completed? Which steps bother you?  \n",
    "\n",
    "我覺得我完成了:  \n",
    "1A, 1B, 1C  \n",
    "2A, 2B, 2C, 2D  \n",
    "3A, 3B, 3C  \n",
    "4A, 4B  \n",
    "5A, 5B, 5C, 5D  \n",
    "\n",
    "沒有完成的部分有: 3D、4C、5E  \n",
    "\n",
    "3D. (5pts) Try to find out the fastest character, draw a red rectangle around the fastest character, and show the output images in the \"find_the_fastest_character\" window.  \n",
    "因為我在處理3D，找出最快的角色的過程中RAM一直不夠，導致影片跑不動或是程式崩潰，這可能由於我撰寫的程式碼有瑕疵，但由於時間因素，我沒有處理這個bug，最終將他從我的程式碼中移除。  \n",
    "\n",
    "4C. (5pts) Try to detect two odd character who face the opposite direction from everyone else, draw a red rectangle around each of the two character, and show the output images in the \"find_two_odds\" window.  \n",
    "我寫了一段程式來找出不同optical flows，希望藉此找到那些跟其他人不同的角色，但寫完之後準確度不高，會偵測到錯誤的人物，因此我最終將他從我的程式碼中移除。  \n",
    "\n",
    "5E. In additon to translation, can your method correctly handle rotated (5pts bonus) and scaled (5pts bonus) hand gestures?  \n",
    "我使用的方法是取出五指指尖的y座標，並藉由五指指尖的高低關係來得出當前手勢為何。然而如果手勢旋轉，舉例來說Victory倒過來，那麼其高低關係就會倒過來，如此一來程式的辨識就會出錯，我想到的解決辦法有計算手指之間的距離、計算手掌的旋轉角度，但我寫完之後發現程式有許多bug，在許多情況下會出錯，因此我選擇了原本的撰寫方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878500c5-31e5-4a77-a5da-b440a9c58d81",
   "metadata": {},
   "source": [
    "7.Any suggestion to teaching assistants to improve this class? Any suggestion to teacher to improve this class?  \n",
    "\n",
    "我覺得整體的上課進度是適中的，老師前半段上課採用講解觀念，後半段採用程式碼範例講解，最後再出一題程式題目讓我們練習，藉由每周都有練習到題目，讓我更了解電腦視覺相關的處理。隨著課程進度的深入，很多題目是需要上課時專心聽講，融會貫通後才有辦法寫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9dcd84-9734-48b2-b3e5-db43074d4707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer_Vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
